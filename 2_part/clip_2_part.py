# -*- coding: utf-8 -*-
"""clip_2_part.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18Q9VMGqTJWkyy1UCB2DdzlynjF0rq1TC
"""

from torchvision.datasets import Imagenette
import numpy as np
from torch.utils.data import DataLoader
from transformers import CLIPProcessor, CLIPModel
import torch
import tqdm
from sklearn.metrics import accuracy_score, f1_score

model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

dataset = Imagenette(root = './data', split = 'val', download = True)
class_names = []

for names in dataset.classes:
  class_names.append(names[0])

prompts = [f"a photo of {name}" for name in class_names]

y_true = []
y_pred = []

text_inputs = processor(text = prompts, return_tensors = 'pt', padding = True)
text_features = model.get_text_features(**text_inputs).pooler_output
text_features = text_features / text_features.norm(dim = 1, keepdim = True)

for image, label in tqdm.tqdm(dataset, desc = 'Total'):
  image_inputs = processor(images = image, return_tensors = 'pt', padding = True)
  image_features = model.get_image_features(**image_inputs).pooler_output
  image_features = image_features / image_features.norm(dim = 1, keepdim = True)

  similarity = image_features @ text_features.T
  pred = similarity.argmax(dim = 1)

  y_pred.append(pred)
  y_true.append(label)

accuracy = accuracy_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred, average = 'micro')

print(f"\nAccuracy = {100 * accuracy:.2f}%")
print(f"F1 = {f1:.2f}")